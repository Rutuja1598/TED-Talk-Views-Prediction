{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1698684054407}],"collapsed_sections":["w6K7xa23Elo4","mDgbUHAGgjLW","HhfV-JJviCcP","nA9Y7ga8ng1Z","dauF4eBmngu3","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - TED Talk Views Prediction\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Regression\n","##### **Contribution**    - Individual"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["In an age where knowledge-sharing has become increasingly important, TED, a nonprofit organization founded in 1984, stands as a beacon for the dissemination of innovative and inspiring ideas. TED's mission is to connect experts across various fields, be it Technology, Entertainment, or Design, with a global audience. Over the years, it has hosted talks by luminaries such as Al Gore, Jimmy Wales, Shahrukh Khan, and Bill Gates. As of 2015, TED, along with its associated TEDx chapters, had made over 2000 talks available for free to the public, making it a vital resource for intellectual enrichment.\n","\n","The core objective of this project is to construct a predictive model that can estimate the number of views a TED talk will garner once uploaded to the TEDx website. This predictive model carries significant implications for TED as it helps the organization better understand the factors influencing the popularity of its talks and provides insights into how to engage its audience more effectively."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["This project seeks to predict the number of views TED talks will receive on the TEDx website, a task crucial for content curation, audience engagement, and resource allocation. The challenge lies in handling diverse data, accounting for unpredictable viewership factors, and creating an interpretable model. The primary objectives include building an accurate predictive model and understanding the influential factors behind talk views. The project's scope covers data processing, feature engineering, model selection, and rigorous performance evaluation, aligning with TED's mission of sharing impactful ideas globally."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n",")\n","from sklearn.preprocessing import (\n","    LabelEncoder,\n","    StandardScaler,\n","    OneHotEncoder,\n","    MinMaxScaler,\n",")\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.utils import resample\n","from scipy import stats\n","import string\n","import re\n","import nltk\n","from nltk.corpus import stopwords, wordnet\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk import pos_tag\n","import random\n","!pip install contractions\n","import contractions\n","from tqdm import tqdm\n","import collections\n","import statsmodels.api as sm\n","from scipy.stats import f_oneway\n","from sklearn.impute import SimpleImputer\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"d0B9jOgXfw0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Dataset\n","csv_file_path = '/content/drive/MyDrive/Project/TED Talk Views Prediction/data_ted_talks.csv'\n","\n","dataset = pd.read_csv(csv_file_path)"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","print(\"\\nFirst 5 rows of the dataset:\")\n","print(dataset.head())"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","dataset.shape"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","print(\"Dataset Information:\")\n","print(dataset.info())"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","dataset.duplicated().sum()"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Countmissing_values = dataset.isnull().sum()\n","missing_values = dataset.isnull().sum()\n","# Display the count of missing values for each column\n","print(\"Missing Values Count per Column:\")\n","print(missing_values)"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(dataset.isnull(), cbar=False, cmap='viridis')\n","plt.title('Missing Values Heatmap')\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Data Structure: The dataset comprises 4,005 rows and 19 columns, with each row representing a unique TED talk. It contains a diverse range of data types, including integers, floats, and objects (textual data).\n","\n","Features: The dataset includes features such as talk titles, speaker information, recording and publication dates, event details, language attributes, view counts, and metadata about talks. Additionally, textual features like talk descriptions, transcripts, and related talks were provided.\n","\n","Data Completeness: Missing values were observed in several columns, including speaker-related details, date information, and the number of comments. The missing values varied in frequency and significance, requiring consideration during data preprocessing.\n","\n","Duplicate Values: The dataset was found to be free of duplicate entries, which is crucial for maintaining data integrity and ensuring accurate analyses.\n","\n","Textual Information: The presence of text-based columns, such as descriptions and transcripts, suggests the potential for natural language processing (NLP) tasks, sentiment analysis, or text-based feature engineering.\n","\n","Numerical Features: Numeric columns, such as view counts and duration, are available for quantitative analysis, and these may be crucial for building a predictive model to estimate talk views.\n","\n","Categorical Features: Categorical features like native language and event categories could be useful for categorization and segmentation tasks.\n","\n","Time Series Data: The dataset contains date-related features, which could facilitate time series analysis or trend identification.\n","\n","Data Quality: The dataset has generally good data quality, but some inconsistencies, missing values, and variations need to be addressed during data preprocessing to ensure the reliability and accuracy of analyses and models."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","print(dataset.columns)"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","print(dataset.describe())"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["talk_id: A unique identifier for each TED talk.\n","\n","title: The title of the TED talk.\n","\n","speaker_1: The primary speaker of the talk.\n","\n","all_speakers: Information about all the speakers involved in the talk.\n","\n","occupations: Occupations or roles of the speakers, describing their professional backgrounds.\n","\n","about_speakers: Information about the background and expertise of the speakers.\n","\n","views: The number of views the talk has received on the TEDx website.\n","\n","recorded_date: The date when the talk was recorded.\n","\n","published_date: The date when the talk was published on the TEDx website.\n","\n","event: The event or conference at which the talk was presented.\n","\n","native_lang: The native language of the talk.\n","\n","available_lang: The languages for which subtitles are available for the talk.\n","\n","comments: The number of comments made by viewers on the talk.\n","\n","duration: The duration of the talk in seconds.\n","\n","topics: Topics associated with the talk, representing the themes or subject matter.\n","\n","related_talks: IDs of related talks, indicating talks that are related to the current one.\n","\n","url: The URL that provides access to the specific talk on the TED website.\n","\n","description: A brief description or summary of the talk's content.\n","\n","transcript: The full transcript of the talk, which may contain the spoken content of the talk."],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for column in dataset.columns:\n","    unique_values = dataset[column].unique()\n","    print(f'Unique values for column \"{column}\":')\n","    for value in unique_values:\n","        print(value)\n","    print(f'Total unique values: {len(unique_values)}')\n","    print('\\n')"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Drop rows with missing values in 'recorded_date'\n","dataset = dataset.dropna(subset=['recorded_date'])"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill missing values in 'about_speakers' and 'occupations' with a default value\n","dataset['about_speakers'].fillna('Unknown', inplace=True)\n","dataset['occupations'].fillna('Unknown', inplace=True)"],"metadata":{"id":"0cCWac9atyQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill missing values in 'comments' with zeros\n","dataset['comments'].fillna(0, inplace=True)"],"metadata":{"id":"q_s9t20_t68a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Type Conversion\n","\n","dataset['recorded_date'] = pd.to_datetime(dataset['recorded_date'])\n","dataset['published_date'] = pd.to_datetime(dataset['published_date'])"],"metadata":{"id":"d75AqqTet-Jf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["Data Manipulations:\n","\n","Handling Missing Values:\n","\n","We addressed missing values in the dataset, with a focus on critical columns.\n","Rows with missing values in the 'recorded_date' column were removed, as this date is essential for time-based analysis.\n","Missing values in 'about_speakers' and 'occupations' were filled with a default value ('Unknown') to ensure the completeness of speaker-related information.\n","Missing values in the 'comments' column were imputed with zeros to indicate no comments.\n","Data Type Conversion:\n","\n","We converted date-related columns, specifically 'recorded_date' and 'published_date,' into datetime objects. This allows for meaningful time-based analysis and visualization.\n","Key Insights:\n","\n","Data Completeness:\n","\n","By handling missing values, we ensured that the dataset is now more complete and suitable for analysis. Removing rows with missing 'recorded_date' values has helped maintain the integrity of time-based analysis.\n","Speaker Information:\n","\n","The dataset often contains 'Unknown' values for 'about_speakers' and 'occupations,' indicating a lack of detailed speaker information for some talks. Understanding this can help us make informed decisions when dealing with speaker-related features.\n","Comments Distribution:\n","\n","The imputation of missing values in the 'comments' column allowed us to maintain data integrity. We observed a wide distribution of comments, ranging from zero to a substantial number, which indicates varying levels of audience engagement with TED talks.\n","Time-Based Analysis:\n","\n","Converting 'recorded_date' and 'published_date' into datetime objects enables us to perform time-based analysis and explore how the timing of TED talk recordings and publications relates to views."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# Group the data by 'native_lang' and count the number of talks in each language\n","language_counts = dataset['native_lang'].value_counts()\n","\n","# Create a bar chart to visualize the distribution\n","plt.figure(figsize=(12, 6))\n","language_counts.plot(kind='bar', color='skyblue')\n","plt.title('Distribution of TED Talks by Native Language')\n","plt.xlabel('Native Language')\n","plt.ylabel('Number of Talks')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","\n","# Show the chart\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I suggested creating a bar chart to visualize the distribution of TED talks by native language because it is a suitable choice for this specific type of data and objective. Here's why I selected this specific chart:\n","\n","Data Type: The 'native_lang' column represents categorical data, where each unique value corresponds to a different native language. Bar charts are commonly used to display the distribution of categorical data, making them an appropriate choice for this dataset.\n","\n","Count of Categories: A bar chart is effective when you want to show the frequency or count of categories (in this case, the number of talks for each native language). It allows viewers to easily compare the counts for different categories.\n","\n","Readability: Bar charts are easy to read and interpret. The x-axis displays the categories (native languages), and the y-axis shows the count or frequency. This makes it straightforward for the audience to understand the distribution of TED talks across languages.\n","\n","Visual Comparison: Bar charts make it simple to visually compare the sizes of different categories. In this context, viewers can quickly identify which native languages have the highest and lowest numbers of talks.\n","\n","Insight Generation: This type of visualization can provide insights into the diversity of languages represented in TED talks, which might be of interest in understanding the global reach and impact of TED's content."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","The chart displaying the distribution of TED talks by native language reveals several insights:\n","\n","English Dominance: English (en) is the dominant language for TED talks, with a significantly higher number of talks compared to other languages. This suggests that English is the most commonly used language for TED presentations, making up a substantial portion of the content.\n","\n","Global Reach: TED's commitment to making ideas accessible globally is evident in the diversity of languages represented in the dataset. While English is prominent, there is a wide array of native languages in which TED talks are conducted, indicating the organization's efforts to reach a worldwide audience.\n","\n","Low Representation: The chart highlights that some languages have relatively lower representation, with only a few or zero talks. This could indicate areas where TED might consider expanding its content to cater to a broader international audience.\n","\n","Multilingual Content: The presence of multiple languages signifies TED's commitment to inclusivity, making talks available to non-English-speaking audiences through translation and subtitles.\n","\n","Data Quality Check: The presence of zero or extremely low counts for some languages might also prompt a data quality check. It's possible that there could be data issues or missing information for certain talks, contributing to the observed distribution."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Audience Reach and Engagement: Understanding the distribution of TED talks by native language allows TED to tailor its content to a global audience. By offering talks in multiple languages, TED can attract and engage a broader and more diverse viewership. This inclusivity can lead to increased user engagement and a larger global following, which can positively impact the business.\n","\n","Cultural Relevance: Providing content in multiple languages demonstrates TED's commitment to cultural relevance and diversity. This approach can strengthen TED's brand and reputation as an organization that respects and values linguistic and cultural diversity.\n","\n","Global Partnerships: TED can leverage its multilingual content to forge partnerships and collaborations with organizations and individuals from various regions. This can lead to opportunities for co-hosted events, cross-promotions, and other mutually beneficial arrangements.\n","\n","Content Monetization: A diverse range of content can attract a larger and more varied audience, potentially enhancing opportunities for content monetization through advertising, sponsorships, and partnerships.\n","\n","Feedback and Localization: Insights from the distribution of talks by language can help TED collect feedback and analytics on the preferences and interests of audiences in different regions. This information can guide content localization and future content creation, enhancing the overall user experience.\n","\n","No Negative Growth Insights:\n","\n","The insights gained from the chart do not inherently lead to negative growth. While there may be lower representation of certain languages, this does not translate directly into negative growth. Instead, it may indicate areas for potential expansion and improvement. TED's commitment to inclusivity and diversity in language representation aligns with its mission, which is unlikely to have negative consequences when approached thoughtfully.\n","\n","However, TED should be mindful of ensuring that low representation of certain languages does not result from data quality issues or neglect of particular regions. In such cases, addressing these concerns can have a positive impact on growth.\n","\n","In summary, the insights gained from the chart primarily support positive business impact by expanding TED's global reach, improving user engagement, and enhancing the organization's commitment to inclusivity and diversity."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Create a histogram to visualize the distribution of TED talk durations\n","plt.figure(figsize=(10, 6))\n","plt.hist(dataset['duration'], bins=30, color='skyblue')\n","plt.title('Distribution of TED Talk Durations')\n","plt.xlabel('Duration (seconds)')\n","plt.ylabel('Number of Talks')\n","\n","# Show the chart\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["Here's why I selected this specific chart:\n","\n","Data Type: The 'duration' column represents continuous numeric data, indicating the length of TED talks in seconds. A histogram is an appropriate choice for visualizing the distribution of such data.\n","\n","Frequency Distribution: A histogram allows you to see how the values are distributed across different duration ranges. It provides insights into the central tendency, spread, and presence of any outliers in the data.\n","\n","Identification of Patterns: By creating a histogram, you can easily identify patterns in the distribution of TED talk durations, such as whether they follow a normal distribution, have multiple peaks, or exhibit skewness.\n","\n","Understanding Typical Duration: This type of visualization helps you understand what the typical duration of TED talks is in your dataset and whether there are any talks that significantly deviate from the norm.\n","\n","Visual Comparison: A histogram is a widely used chart for comparing the distribution of values, making it easy to identify any unusual trends or features in the dataset."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","Diverse Talk Durations: The histogram reveals that TED talks have a wide range of durations, with the majority of talks falling within a particular duration range. This diversity in talk durations suggests that TED content accommodates various topics and speaker styles, allowing for both short and long presentations.\n","\n","Peak Duration Range: There is a clear peak in the histogram, indicating a common duration range for TED talks. This peak suggests that a significant number of talks are of similar duration, which can be informative for event planning and audience expectations.\n","\n","Longer Talks: The right tail of the histogram shows that there are talks with longer durations, indicating that TED is open to hosting more extended discussions when necessary. These longer talks may be related to more in-depth or complex topics.\n","\n","Short Talks: The left tail of the histogram indicates the presence of shorter talks, possibly used for concise and impactful presentations or introductions to specific concepts.\n","\n","Outliers: The histogram allows for the identification of potential outliers, talks with durations significantly different from the norm. These outliers can be of interest for further analysis, as they may represent unique or exceptional cases.\n","\n","Typical Duration: The histogram provides insights into the typical duration of TED talks in the dataset, helping TED organizers and viewers understand the expected length of a standard TED talk.\n","\n","Audience Engagement: Talk duration is a crucial factor in audience engagement. The distribution suggests that TED effectively manages the balance between providing concise talks that capture attention and longer talks that delve into complex subjects."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Audience Engagement: Understanding the distribution of talk durations allows TED to tailor its content to the preferences and attention spans of its audience. This can result in higher audience engagement, as talks of various lengths cater to different viewers with distinct needs.\n","\n","Content Planning: TED can use insights into the typical duration and distribution to plan content more effectively. This helps ensure that the duration of each talk aligns with the depth of the topic and the target audience.\n","\n","Diverse Topics: The wide range of talk durations signifies TED's versatility in accommodating diverse topics and presentations. This diversity is appealing to a broad audience, encouraging them to explore a range of subjects.\n","\n","Monetization: Content that caters to varying attention spans and interests can enhance opportunities for monetization through ads, sponsorships, and memberships.\n","\n","Tailored Events: TED can plan events with a mix of talk durations to offer a varied and engaging experience for attendees.\n","\n","No Negative Growth Insights:\n","\n","The insights from the duration distribution don't inherently lead to negative growth. However, TED should be cautious about managing outliers, ensuring they align with the content's quality and relevance to maintain a high standard. Overly long or short talks that compromise content quality may negatively affect viewer satisfaction. Therefore, while longer or shorter talks are accommodated, they must still provide value and maintain TED's reputation for quality content.\n","\n","In summary, the insights gained from the distribution of TED talk durations support positive business impact by enhancing audience engagement, content planning, diversity, and monetization opportunities. TED's commitment to inclusivity in terms of talk lengths can be advantageous without posing direct negative growth risks, provided that content quality and relevance are maintained."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","# Group the data by 'native_lang' and count the number of talks in each language\n","language_counts = dataset['native_lang'].value_counts()\n","\n","# Create a pie chart to visualize the distribution\n","plt.figure(figsize=(8, 8))\n","plt.pie(language_counts, labels=language_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(language_counts))))\n","plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n","plt.title('Distribution of TED Talks by Native Language')\n","\n","# Show the chart\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["Proportion Display: A pie chart is ideal when you want to show how a whole (in this case, the total number of TED talks) is distributed among various categories (native languages). It provides a clear representation of the proportion of each category relative to the whole.\n","\n","Relative Comparisons: The pie chart makes it easy to compare the sizes of different language categories at a glance. Viewers can immediately see the distribution of talks by language, which is useful for understanding the composition of TED's content.\n","\n","Percentage Information: The percentage labels on the pie chart slices provide specific information about the contribution of each language. This makes it clear to viewers what portion of TED talks is in each language.\n","\n","Concise Visualization: Pie charts are concise and easy to understand, making them effective for communicating a high-level overview of data.\n","\n","Linguistic Diversity: In the context of TED talks, the pie chart highlights the linguistic diversity and the organization's commitment to offering content in multiple languages.\n","\n","In summary, a pie chart is a suitable choice for visualizing the distribution of TED talks by native language because it effectively conveys the proportion of talks in each language relative to the total, making it easy for viewers to grasp the linguistic diversity of TED's content."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","English Dominance: The pie chart clearly illustrates the dominance of English as the primary language for TED talks. The sizable portion of the chart occupied by English indicates that a significant majority of TED talks are presented in English.\n","\n","Multilingual Content: While English is predominant, the presence of multiple language segments shows that TED offers content in various native languages. This reflects TED's commitment to inclusivity and its mission to make ideas accessible to global audiences.\n","\n","Linguistic Diversity: The pie chart highlights the linguistic diversity of TED's content, with different languages represented. This diversity makes TED accessible and appealing to audiences from various linguistic backgrounds.\n","\n","Language Representation: By viewing the chart, viewers can quickly identify the distribution of talks in other languages. This can be informative for viewers who prefer talks in specific languages and can navigate to those talks easily.\n","\n","Potential for Growth: The chart also suggests potential areas for growth and expansion. While English is dominant, other languages may have room for growth in terms of the number of talks, catering to specific language-speaking audiences.\n","\n","Content Localization: TED's commitment to language diversity suggests the organization's willingness to localize content to reach broader audiences. Localization can lead to greater audience engagement and global reach.\n","\n","In summary, the pie chart provides insights into the linguistic diversity of TED talks, emphasizing the dominance of English while showcasing TED's commitment to making ideas accessible in multiple languages. This diversity contributes to TED's global appeal and accessibility to a wide range of audiences."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Audience Reach: Understanding the linguistic diversity of TED's content allows TED to effectively reach and engage a global audience. This can positively impact business by increasing the organization's reach and audience engagement.\n","\n","Inclusivity: The representation of various languages signifies TED's commitment to inclusivity. This inclusive approach can enhance TED's reputation and attract a more diverse viewership, leading to positive business outcomes.\n","\n","Catering to Language Preferences: Knowledge of language distribution enables TED to cater to viewers' language preferences. This can lead to increased viewership and user satisfaction, positively impacting the business.\n","\n","Localization Opportunities: Insights into language distribution can guide TED in making localized content, which can be appealing to specific language-speaking audiences. Localization can result in business growth and greater global reach.\n","\n","Global Partnerships: The ability to offer content in multiple languages opens doors for global partnerships, collaborations, and expansion into international markets, all of which can contribute to business growth.\n","\n","No Negative Growth Insights:\n","\n","The insights from the language distribution pie chart don't inherently lead to negative growth. Instead, they highlight TED's commitment to linguistic diversity and inclusivity, which align with TED's mission of sharing ideas globally. There is no direct negative impact associated with this insight.\n","\n","However, it's crucial for TED to ensure that the quality and relevance of content in different languages are maintained to uphold its reputation for high-quality talks."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","# Set the number of top events to display\n","top_n_events = 10  # You can adjust this number as needed\n","\n","# Group the data by 'event' and count the number of talks in each event\n","event_counts = dataset['event'].value_counts()\n","\n","# Sort the events by the number of talks in descending order\n","event_counts = event_counts.sort_values(ascending=False)\n","\n","# Select the top N events\n","top_event_counts = event_counts.head(top_n_events)\n","\n","# Create a bar chart to visualize the distribution of the top events\n","plt.figure(figsize=(12, 8))\n","top_event_counts.plot(kind='bar', color='lightblue')\n","plt.title(f'Top {top_n_events} TED Events/Conferences by Number of Talks')\n","plt.xlabel('Event/Conference')\n","plt.ylabel('Number of Talks')\n","\n","# Show the chart\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["Frequency Representation: A bar chart is ideal when you want to represent the frequency or count of occurrences for different categories (in this case, TED events or conferences). Each bar represents a specific event, and its height corresponds to the number of talks associated with that event.\n","\n","Comparative Analysis: A bar chart allows for easy visual comparison of the number of talks across different events. Viewers can quickly identify which events have hosted the most talks and compare the distribution.\n","\n","Top N Selection: By plotting the top N events with the most talks, the chart simplifies the visualization and focuses on the most significant contributors, making it easier to identify trends.\n","\n","Ranking: The ordering of bars in descending order allows for clear ranking, making it evident which events have the highest and lowest talk counts.\n","\n","Readability: Bar charts are highly readable and straightforward, making them suitable for conveying the distribution of discrete categories to a wide audience."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","Event Popularity: The bar chart reveals the popularity and significance of various TED events and conferences. Some events have hosted a considerably higher number of talks than others, indicating their importance within the TED community.\n","\n","Top Events: By focusing on the top events with the most talks, the chart identifies the key events that have contributed significantly to TED's content. This insight is valuable for event organizers and audiences interested in specific TED events.\n","\n","Diversity: While certain events stand out in terms of talk count, the presence of multiple events with substantial talk contributions suggests TED's commitment to diversity and the exploration of a wide range of topics.\n","\n","Content Curation: The distribution of talks by event reflects TED's content curation strategy. Different events may have specific themes or areas of focus, and this is evident in the distribution of talks across events.\n","\n","Opportunities for Growth: The chart can highlight events with fewer talks, potentially indicating opportunities for growth and expansion. TED may consider increasing the number of talks at events that show potential.\n","\n","Event Influence: The chart showcases the influence of certain events within the TED community and the impact they have on the dissemination of ideas and knowledge.\n","\n","In summary, the bar chart provides insights into the distribution of TED talks across different events, offering visibility into the popularity of specific events, diversity in content, opportunities for growth, and the impact of events on TED's mission of sharing innovative ideas. These insights can inform content curation, event planning, and audience engagement strategies."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["preferences and interests of specific audiences. Positive business outcomes include higher viewership and audience satisfaction.\n","\n","Event Promotion: TED can leverage insights about the most popular events to promote and market those events more effectively. This can lead to increased event attendance, sponsorships, and revenue generation.\n","\n","Sponsorship and Partnerships: Events with high talk counts can attract more sponsors and potential partners, leading to revenue opportunities and business growth.\n","\n","Audience Engagement: By focusing on top events and tailoring content for these events, TED can enhance audience engagement and encourage event attendees to explore other TED content, contributing to a positive business impact.\n","\n","Event Growth Opportunities: The chart can help identify events with fewer talks, providing opportunities for growth. Expanding the number of talks at these events can lead to increased audience reach and event attendance, benefiting TED's business.\n","\n","Content Monetization: Popular events with a substantial talk count can be monetized more effectively through ads, sponsorships, and memberships, enhancing revenue potential.\n","\n","No Negative Growth Insights:\n","\n","The insights from the distribution of talks by event don't inherently lead to negative growth. Instead, they provide valuable information for optimizing content curation, event promotion, and audience engagement. There are no direct negative impacts associated with this insight."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","# Create a scatter plot to visualize the relationship\n","plt.figure(figsize=(10, 6))\n","plt.scatter(dataset['duration'], dataset['views'], alpha=0.5, color='blue')\n","plt.title('Relationship between TED Talk Duration and Views')\n","plt.xlabel('Duration (seconds)')\n","plt.ylabel('Number of Views')\n","\n","# Show the chart\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["Correlation Assessment: A scatter plot is an effective choice when you want to assess the relationship or correlation between two numeric variables, such as talk duration and the number of views. It allows for a visual examination of how changes in one variable relate to changes in another.\n","\n","Continuity Representation: Scatter plots are ideal for displaying continuous data, which is the case for both talk duration (measured in seconds) and view counts. They help viewers understand how these continuous variables interact.\n","\n","Pattern Identification: Scatter plots enable the identification of patterns, trends, clusters, or outliers in the data. This can provide insights into whether there is a relationship between the length of a talk and its popularity.\n","\n","Quantitative Assessment: Viewers can quantitatively assess the data points' distribution, dispersion, and any potential linear or non-linear relationships. This is particularly important when exploring variables that may impact user engagement, such as the duration of content and the number of views.\n","\n","Visual Clarity: Scatter plots are easy to understand and interpret, making them a straightforward choice for presenting the relationship between two numeric variables.\n","\n","In summary, a scatter plot is a suitable choice for visualizing the relationship between the duration of TED talks and the number of views because it facilitates the exploration of correlations, pattern identification, and quantitative assessment, which are important for understanding user engagement with content."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","No Clear Linear Relationship: The scatter plot shows a dispersed distribution of data points, indicating that there is no clear linear relationship between the duration of TED talks and the number of views. In other words, the length of a talk does not directly determine its popularity in a linear fashion.\n","\n","Varied Talk Lengths: TED talks vary in duration, with some being relatively short and others longer. Despite the variation, talks of different lengths receive a wide range of view counts, suggesting that viewership is influenced by other factors beyond talk duration.\n","\n","Popularity of Short Talks: While there is no strict linear correlation, the scatter plot highlights that shorter talks (with shorter durations) also receive a significant number of views. Some shorter talks have garnered high popularity, demonstrating that concise content can resonate with audiences.\n","\n","Diversity in Content: The plot reflects the diversity of TED content, with talks of different durations appealing to various audiences. TED's commitment to diverse topics and ideas is evident in the range of talk durations and their associated views.\n","\n","Audience Engagement: The lack of a linear correlation suggests that audience engagement is influenced by factors beyond talk duration, such as topic, speaker, and the presentation style. This insight underscores the importance of captivating content and effective communication.\n","\n","Opportunities for Impact: TED can continue to leverage the diversity of talk durations to reach a wide range of audiences. By offering both short and long talks, TED can cater to viewers with different preferences and optimize content delivery."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Content Diversity: The diversity of TED talk durations, as highlighted in the scatter plot, is an asset. It allows TED to cater to a wide range of audience preferences. Viewers can choose talks that fit their available time and interests. This diversity can lead to increased viewership and audience satisfaction, contributing to a positive impact on business.\n","\n","Audience Engagement: By recognizing that the length of a talk is not the sole factor driving viewership, TED can focus on the quality of content, speaker expertise, and the impact of ideas presented. This can enhance audience engagement and loyalty, leading to positive business outcomes.\n","\n","Content Monetization: The diverse content offerings, as revealed in the plot, can be leveraged for content monetization. Different talks can be monetized through various models, such as ads, sponsorships, and memberships, potentially increasing revenue.\n","\n","Customized Content: TED can continue to offer talks of varying durations and cater to different audience segments. By providing customized content experiences, TED can maintain and expand its global audience, furthering its mission.\n","\n","No Negative Growth Insights:\n","\n","The insights from the scatter plot do not lead to negative growth. Instead, they provide valuable information for optimizing content diversity, audience engagement, and content monetization. There are no direct negative impacts associated with this insight."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Group the data by 'topics' and count the number of talks in each topic\n","topic_counts = dataset['topics'].str.split(', ').explode().value_counts()\n","\n","# Select the top N topics for better readability\n","top_topics = topic_counts.head(10)\n","\n","# Create a pie chart to visualize the distribution of talks by topic\n","plt.figure(figsize=(10, 10))\n","plt.pie(top_topics, labels=top_topics.index, autopct='%1.1f%%', startangle=140)\n","plt.title('Distribution of TED Talks by Topic')\n","plt.axis('equal')  # Equal aspect ratio ensures a circular pie chart\n","\n","# Show the chart with rotated labels\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["Composition Representation: A pie chart is ideal for representing the distribution of talks across different topics because it provides a clear and intuitive view of how the whole (all TED talks) is divided into its parts (topics). This aids in understanding the relative prevalence of different themes.\n","\n","Percentage Visualization: Pie charts readily show the proportions of each part relative to the whole. In this case, it's beneficial for understanding the percentage of talks devoted to various topics, making it easy to identify dominant and minority themes.\n","\n","Limited Categories: When dealing with a limited number of categories or topics, as is the case with TED talks, a pie chart is a concise and visually effective choice. It enables viewers to quickly grasp the main themes without excessive complexity.\n","\n","Comparison: Viewers can easily compare the sizes of different topics in the pie chart, identifying which topics have a significant presence and which are less prominent. This can aid in identifying trends and areas of interest.\n","\n","Engagement with Themes: The chart can engage viewers by highlighting the diversity of TED content. It allows for quick insights into the wide range of ideas and subjects covered in TED talks."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","Diversity of Topics: The pie chart highlights the wide diversity of topics covered in TED talks. TED is not limited to a specific subject but rather encompasses a broad range of themes and ideas, as evidenced by the numerous topics represented in the chart.\n","\n","Prominent Themes: While many topics are present, a few themes stand out as prominently featured. These dominant themes capture a significant portion of the talks, indicating that certain subjects resonate more with TED's audience.\n","\n","Balanced Distribution: The chart also shows a relatively balanced distribution of talks across various topics. While some topics have a larger presence, there is no extreme skew toward a single theme, reinforcing TED's commitment to diversity.\n","\n","Popular Areas of Interest: Viewers can identify popular areas of interest by looking at the larger slices of the pie. These topics may have a broader appeal and attract more viewers, reflecting trends in audience engagement.\n","\n","Opportunities for Exploration: Smaller slices represent niche topics that, while less prevalent, still have a place in TED's content library. These areas offer opportunities for viewers to explore unique and specialized subjects.\n","\n","Alignment with TED's Mission: The diverse range of topics aligns with TED's mission to spread innovative and inspiring ideas across various fields, ensuring that the platform continues to deliver on its core purpose."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Diverse Audience Engagement: The diversity of topics covered in TED talks ensures that TED can engage a wide and varied audience. This diversity aligns with TED's mission to share innovative and inspiring ideas across various fields, and it can lead to positive business outcomes by attracting a broad audience.\n","\n","Viewer Satisfaction: By offering a diverse range of topics, TED can cater to the interests of a broad spectrum of viewers. Viewer satisfaction and engagement are essential for the success of a content platform, and a diverse range of topics can contribute positively to this.\n","\n","Content Monetization: The diversity of content themes opens up opportunities for content monetization. Different topics may appeal to different sponsorships, partnerships, or targeted advertising, potentially increasing revenue.\n","\n","Global Reach: TED's diverse range of topics is not limited by geographic or cultural boundaries. This can help TED expand its global reach and impact, fostering positive growth.\n","\n","Brand Reputation: A platform known for its diversity and inclusivity in content can enhance its brand reputation and positive image in the eyes of viewers and stakeholders.\n","\n","No Negative Growth Insights:\n","\n","The insights from the pie chart do not lead to negative growth. Instead, they provide valuable information for optimizing content diversity, audience engagement, content monetization, and brand reputation. There are no direct negative impacts associated with this insight."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Extract the year from the 'recorded_date' column\n","dataset['recorded_year'] = dataset['recorded_date'].dt.year\n","\n","# Group the data by 'recorded_year' and count the number of talks recorded each year\n","recorded_year_counts = dataset['recorded_year'].value_counts().sort_index()\n","\n","# Create a bar chart to visualize the distribution of talks by the year they were recorded\n","plt.figure(figsize=(12, 6))\n","recorded_year_counts.plot(kind='bar', color='skyblue')\n","plt.title('Distribution of TED Talks by Recorded Year')\n","plt.xlabel('Recorded Year')\n","plt.ylabel('Number of Talks')\n","\n","# Show the chart\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["Temporal Trends: A bar chart is well-suited for showing how the number of talks recorded each year has evolved over time. It allows viewers to easily identify trends and patterns in the data.\n","\n","Yearly Distribution: This chart can provide insights into the growth and fluctuation of TED talks over the years. It's essential for understanding the historical development of the TED platform.\n","\n","Comparative Analysis: Viewers can compare the number of talks recorded in different years, identifying peaks, declines, or stability in content creation. This facilitates data-driven decision-making.\n","\n","Engagement and Relevance: Observing the distribution of talks by recorded year can help assess the audience's ongoing engagement with older talks and the relevance of past content.\n","\n","Content Planning: TED can use this information to plan content for future years, adjusting topics or themes based on historical trends.\n","\n","Audience Retention: Understanding how older talks continue to attract views can inform strategies to retain and engage the audience over time."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["Insight from the Chart:\n","\n","Historical Growth: The chart shows that TED talks have experienced significant growth over the years. The number of talks recorded annually has generally increased, indicating TED's expansion as a platform for sharing ideas.\n","\n","Yearly Peaks: The chart reveals specific years with noticeable peaks in the number of recorded talks. These peaks may be associated with significant events, partnerships, or content strategies that led to a surge in content creation.\n","\n","Steady Growth: Despite fluctuations, there is a steady upward trend in the number of talks recorded. This suggests TED's continuous commitment to its mission of spreading innovative and inspiring ideas.\n","\n","Sustainability: The presence of talks recorded in earlier years, such as the early 2000s, shows that TED's older content remains relevant and continues to attract viewers. This underscores the sustainability and enduring appeal of TED's content.\n","\n","Data-Driven Decision-Making: TED can use this historical data to make informed decisions about content planning, partnerships, and audience engagement strategies. Insights from the chart can guide future initiatives.\n","\n","Content Variety: The chart highlights that TED has maintained a diverse range of topics throughout its history, appealing to a broad audience with different interests."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Data-Driven Decision-Making: The insights derived from the chart provide TED with historical data on the growth and trends in content creation. This data can be instrumental in shaping content planning, audience engagement strategies, and resource allocation. Informed decisions based on data are more likely to yield positive outcomes.\n","\n","Content Sustainability: The chart highlights the enduring appeal and sustainability of older content. Talks recorded in previous years continue to attract viewers, contributing positively to audience retention and engagement. This indicates that TED's content has a lasting impact.\n","\n","Content Diversity: The chart reflects TED's commitment to maintaining a diverse range of topics over the years. A diverse content library appeals to a broad and varied audience, fostering viewer satisfaction and engagement.\n","\n","Potential Partnerships: TED can leverage the insights to identify peak years in content creation and explore partnerships, sponsorships, or promotions during those times. This can lead to increased visibility and revenue opportunities.\n","\n","No Negative Growth Insights:\n","\n","The insights from the chart do not lead to negative growth. Instead, they provide valuable historical data that can be harnessed to make informed decisions and enhance content planning, audience engagement, and partnerships. There are no direct negative impacts associated with this insight."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Extract the year and month from the 'recorded_date' column\n","dataset['recorded_year_month'] = dataset['recorded_date'].dt.to_period('M')\n","\n","# Group the data by 'recorded_year_month' and calculate the total views per month\n","views_by_month = dataset.groupby('recorded_year_month')['views'].sum()\n","\n","# Create a line chart to visualize the trends in TED talk views over time\n","plt.figure(figsize=(12, 6))\n","views_by_month.plot(kind='line', color='royalblue', marker='o')\n","plt.title('Trends in TED Talk Views Over Time')\n","plt.xlabel('Year-Month')\n","plt.ylabel('Total Views')\n","\n","# Show the chart\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["Temporal Trends: A line chart is well-suited for displaying how a numeric variable (in this case, TED talk views) changes over time. It is ideal for revealing temporal patterns, trends, and fluctuations.\n","\n","Viewership Growth: The chart can clearly show whether TED's viewership has grown or declined over the years. This insight is valuable for assessing the platform's overall impact and engagement.\n","\n","Granular Detail: By using the recorded year and month as the time unit, the line chart provides granular insights into monthly viewership trends. This level of detail can reveal seasonal patterns or events that influence viewership.\n","\n","Visualizing Data Trends: Line charts excel at visualizing data trends and making them easily interpretable. They connect data points over time, allowing viewers to see the trajectory of viewership.\n","\n","Comparative Analysis: The line chart can facilitate comparative analysis by showing multiple lines for different years or specific events. This enables the identification of changes or anomalies in viewership.\n","\n","Audience Engagement: Understanding how viewership has changed over time is crucial for assessing audience engagement and the long-term impact of TED's content."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["Insights from the Chart:\n","\n","Steady Growth: The line chart demonstrates a consistent upward trend in TED talk views over time. This indicates that TED's online presence and viewership have grown steadily, reflecting the enduring appeal of its content.\n","\n","Seasonal Fluctuations: The chart reveals periodic fluctuations in viewership. These fluctuations might be related to factors such as the timing of major events, conferences, or the release of specific talks. Identifying these patterns can help TED strategically plan content releases.\n","\n","Impactful Talks: Sudden spikes in the line chart represent talks that have attracted a significantly higher number of views. These spikes can be correlated with specific talks that resonated strongly with the audience, contributing to TED's growth.\n","\n","Long-Term Relevance: The presence of older data points in the chart, which continue to contribute to views, highlights the long-term relevance of TED's content. It indicates that viewers continue to discover and engage with older talks.\n","\n","Data-Driven Decision-Making: TED can use the insights from the chart to make data-driven decisions regarding content planning, marketing strategies, and partnerships. Recognizing patterns in viewership trends allows for targeted efforts.\n","\n","Content Strategy: The chart can guide TED in shaping its content strategy by identifying the most and least successful periods in terms of viewership. This information can influence the selection of topics and speakers."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Steady Growth: The chart illustrates a consistent upward trend in TED talk views, indicating that TED's online presence is flourishing. This growth signifies a positive impact, as it reflects an expanding and engaged audience.\n","\n","Data-Driven Decision-Making: The insights from the chart provide TED with data that can guide content planning, marketing strategies, and resource allocation. Informed decisions based on viewership trends are more likely to yield positive results.\n","\n","Seasonal Fluctuations: Understanding the seasonal fluctuations in viewership allows TED to strategically plan content releases during periods of high engagement. This can lead to increased viewership during specific times of the year.\n","\n","Impactful Talks: Identifying talks that attracted significantly higher views can inform content selection and speaker invitations. Replicating the success of impactful talks can contribute positively to viewership.\n","\n","Long-Term Relevance: The chart highlights the enduring relevance of older talks, indicating that TED's content has a lasting impact. This is a positive reflection of TED's mission to share valuable ideas.\n","\n","Content Strategy: The insights from the chart can guide TED's content strategy, enabling them to focus on topics and speakers that resonate strongly with the audience.\n","\n","No Negative Growth Insights:\n","\n","The insights from the chart do not lead to negative growth. Instead, they provide valuable historical data that can be leveraged to enhance content planning, marketing, and partnerships. There are no direct negative impacts associated with this insight."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","# Extract and flatten the list of topics from the 'topics' column\n","all_topics = [topic for topics_list in dataset['topics'] for topic in eval(topics_list)]\n","\n","# Create a list of the top N most common topics\n","top_topics = [topic for topic, count in collections.Counter(all_topics).most_common(10)]\n","\n","# Count the occurrences of each top topic in the dataset\n","topic_counts = [all_topics.count(topic) for topic in top_topics]\n","\n","# Create a bar chart to visualize the distribution of top TED talk topics\n","plt.figure(figsize=(12, 6))\n","plt.barh(top_topics, topic_counts, color='royalblue')\n","plt.title('Top 10 TED Talk Topics')\n","plt.xlabel('Number of Talks')\n","plt.ylabel('Topic')\n","\n","# Show the chart\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["Bar Chart for Topic Distribution:\n","\n","Comparative Analysis: A bar chart is ideal for displaying and comparing the frequencies of different categories (in this case, TED talk topics). It allows for a straightforward visual comparison of the number of talks in each category.\n","\n","Topical Insights: The chart provides insights into the most popular TED talk topics. It helps identify which subjects have been most frequently addressed, providing valuable information for content planning.\n","\n","Ranked Representation: By displaying the topics in descending order of frequency, the chart makes it easy to identify the most common and the less common subjects. This ranked representation is valuable for identifying trends.\n","\n","Topical Diversity: TED is known for its diverse range of subjects, and the chart can highlight this diversity by showcasing a wide array of topics. It can also emphasize TED's commitment to exploring various areas of knowledge and expertise.\n","\n","Visual Clarity: The bar chart offers clarity in presenting the data, making it accessible to a broad audience. Each bar represents a topic, and the height of the bar corresponds to the number of talks on that topic.\n","\n","Data-Driven Insights: The chart can guide TED in choosing topics for future talks, addressing gaps in coverage, and maintaining a balance of subject matter."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["Insights from the Chart:\n","\n","Diversity of Topics: The chart showcases the diverse range of topics covered in TED talks. This diversity reflects TED's commitment to exploring a wide array of subjects and areas of expertise.\n","\n","Popular Themes: The chart highlights the most frequently addressed topics in TED talks. These popular themes include technology, science, and creativity, which have a significant presence in TED's content.\n","\n","Society and Culture: Subjects related to society, culture, and human behavior also hold a prominent place in TED talks. This suggests TED's dedication to addressing issues that impact society.\n","\n","Education and Learning: The presence of education and learning as a top topic category underscores TED's role as an educational platform that aims to share knowledge and inspire learning.\n","\n","Health and Well-being: Health and well-being topics are also well-represented, reflecting the interest in personal and societal well-being.\n","\n","Content Planning: The insights from the chart can guide TED in content planning. TED can identify areas with a high demand for talks, as well as subject areas that may require more coverage.\n","\n","Balancing Themes: TED can use these insights to ensure a balanced mix of themes, addressing both popular and less common subjects to cater to a diverse audience."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Diverse Content Attraction: The chart showcases the diverse range of topics covered in TED talks, indicating that TED appeals to a broad audience with varied interests. This diversity is a positive factor as it broadens the platform's reach and engagement.\n","\n","Popular Themes: Identifying the most frequently addressed topics, such as technology, science, and creativity, can help TED understand the areas that resonate most with the audience. This knowledge enables TED to continue producing content on popular themes, driving more views and engagement.\n","\n","Addressing Societal Issues: The presence of topics related to society and culture reflects TED's commitment to addressing important societal issues. This aligns with TED's mission to share ideas and promote positive change.\n","\n","Education and Learning: TED's focus on education and learning as a prominent topic category supports its role as an educational platform. This is a positive impact as it attracts individuals seeking knowledge and personal development.\n","\n","Health and Well-being: The inclusion of health and well-being topics is beneficial as it caters to viewers interested in personal and societal well-being, potentially driving engagement and positive impact in these areas.\n","\n","Content Planning: The insights from the chart can guide TED in content planning, helping them focus on topics with high demand and adjust their content strategy accordingly.\n","\n","No Negative Growth Insights:\n","\n","The insights from the chart do not lead to negative growth. Instead, they provide valuable data for TED to refine its content strategy and maintain a balance of subject matter. There are no direct negative impacts associated with these insights."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","# Create a histogram to visualize the distribution of TED talk durations\n","plt.figure(figsize=(10, 6))\n","plt.hist(dataset['duration'], bins=20, color='skyblue', edgecolor='black')\n","plt.title('Distribution of TED Talk Durations')\n","plt.xlabel('Duration (seconds)')\n","plt.ylabel('Number of Talks')\n","\n","# Show the chart\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.show()"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["Histogram for TED Talk Durations:\n","\n","Distribution Analysis: A histogram is well-suited for analyzing the distribution of continuous data, such as the duration of TED talks. It allows you to see how talk durations are distributed across different time intervals.\n","\n","Central Tendency: By examining the histogram, you can quickly identify the typical or central duration of TED talks. This provides insights into the average length of talks.\n","\n","Variability: The histogram also reveals the variability in talk durations. You can see the range of durations, including any outliers or extreme values.\n","\n","Audience Engagement: Understanding the distribution of talk durations is valuable for managing audience engagement. It helps ensure that TED talks are of an appropriate length to maintain viewer interest.\n","\n","Content Planning: TED can use this information for content planning, ensuring a balance of talk durations that align with audience preferences.\n","\n","Data-Driven Decisions: TED can make data-driven decisions about setting guidelines for talk durations based on the insights gained from the histogram."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["Insights from the Chart:\n","\n","Varied Talk Durations: The histogram reveals a wide range of TED talk durations, indicating that TED hosts talks of various lengths. This diversity in talk durations allows TED to cater to different audience preferences.\n","\n","Peak at Approximately 1,000 Seconds: The histogram shows a peak in the distribution at approximately 1,000 seconds (or around 16-17 minutes). This suggests that TED has a significant number of talks with this specific duration, which may be considered an ideal or typical length for TED talks.\n","\n","Longer Talks: There is a long tail on the right side of the histogram, indicating the presence of longer talks. These talks are likely to be in-depth and may cover complex subjects.\n","\n","Shorter Talks: On the left side of the histogram, there are shorter talks, which are likely to be concise and focused. These shorter talks may be engaging for viewers who prefer more succinct content.\n","\n","Tailored Content: TED's ability to offer talks of varying lengths allows for tailored content delivery. Viewers can choose talks based on their available time and preferences, leading to higher audience engagement.\n","\n","Content Strategy: The insights from the histogram can inform TED's content strategy, helping them maintain a balance of talk durations and ensuring that they continue to meet the expectations of their diverse audience.\n","\n","In summary, the histogram of TED talk durations reveals a diverse distribution of talk lengths, with a peak around 1,000 seconds. TED can use this information to tailor its content and maintain a balance of talk durations, meeting the preferences of its audience and optimizing audience engagement."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["Positive Business Impact:\n","\n","Audience Engagement: The ability to offer talks of varying durations allows TED to cater to a broad and diverse audience. This flexibility enhances audience engagement, as viewers can choose talks that align with their available time and preferences.\n","\n","Content Planning: TED can use the insights from the histogram to fine-tune its content planning. They can ensure a balanced mix of talk durations, meeting the expectations of their audience and providing a variety of content.\n","\n","Viewer Satisfaction: By offering talks of different lengths, TED can increase viewer satisfaction. Some viewers may prefer shorter, concise talks, while others may seek more in-depth, longer discussions. Meeting these preferences positively impacts user satisfaction.\n","\n","Content Relevance: TED can optimize content relevance by tailoring talk durations to specific subjects. Some topics may require longer, more detailed discussions, while others can be effectively conveyed in shorter talks.\n","\n","Data-Driven Decisions: The insights enable TED to make data-driven decisions about setting guidelines for talk durations and content planning, ultimately leading to a more efficient content strategy.\n","\n","There are no negative growth insights associated with the distribution of talk durations. The insights gained from the histogram support TED's mission of sharing ideas effectively and engaging its diverse audience.\n","\n","In summary, the insights from the histogram of talk durations positively impact TED's content strategy, audience engagement, and viewer satisfaction. There are no insights that lead to negative growth; rather, the findings enhance TED's ability to provide relevant and engaging content to a wide range of viewers."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Hypothetical Statement 1:\n","\"TED talks with a longer duration receive more views than talks with shorter durations.\"\n","\n","Hypothetical Statement 2:\n","\"The number of comments on TED talks is significantly influenced by the native language in which the talk is delivered.\"\n","\n","Hypothetical Statement 3:\n","\"TED talks with more speakers receive a higher number of views compared to talks with a single speaker.\""],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): The duration of a TED talk does not significantly impact the number of views it receives.\n","\n","Alternative Hypothesis (H1): The duration of a TED talk significantly impacts the number of views it receives.\n","\n","We will perform hypothesis testing to evaluate whether the duration of TED talks has a significant effect on the number of views they receive."],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Define the dependent variable (views) and independent variable (duration)\n","y = dataset['views']\n","X = dataset['duration']\n","\n","# Add a constant (intercept) to the independent variable\n","X = sm.add_constant(X)\n","\n","# Fit the linear regression model\n","model = sm.OLS(y, X).fit()\n","\n","# Get the summary statistics, including the p-value\n","summary = model.summary()\n","\n","# Extract the p-value\n","p_value = model.pvalues['duration']\n","\n","# Print the p-value\n","print(\"P-Value:\", p_value)"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["To obtain the p-value for the first hypothetical statement, we performed a simple linear regression analysis. The p-value was obtained as part of this regression analysis. In a linear regression analysis, the p-value associated with the coefficient of the independent variable of interest (in this case, the duration of TED talks) is used to test the significance of the relationship between the independent variable and the dependent variable (in this case, the number of views).\n","\n","So, the statistical test used to obtain the p-value was a simple linear regression analysis."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["I chose a simple linear regression analysis as the specific statistical test to evaluate the relationship between the duration of TED talks and the number of views for the following reasons:\n","\n","Nature of the Relationship: Linear regression is appropriate when we want to understand the relationship between a continuous independent variable (duration) and a continuous dependent variable (views). It is commonly used to assess how changes in one variable impact changes in another variable.\n","\n","Hypothesis Testing: Linear regression allows us to perform hypothesis testing on the coefficient of the independent variable. In this case, we can test whether the duration of TED talks has a significant impact on the number of views, which aligns with the stated research hypothesis.\n","\n","Interpretability: Linear regression provides a clear interpretation of the relationship. The coefficient of the duration variable tells us how a one-unit change in duration affects the number of views. The p-value associated with this coefficient informs us about the significance of this relationship.\n","\n","Widely Accepted: Linear regression is a well-established and widely accepted statistical technique for analyzing relationships between variables. It is a common choice for examining the influence of one variable on another in regression analysis.\n","\n","Ease of Implementation: The linear regression analysis can be easily performed using statistical libraries like statsmodels in Python, making it a practical choice for testing the hypothesis.\n","\n","In summary, the choice of a simple linear regression analysis was appropriate for testing the specific research hypothesis, as it allows us to assess the significance and direction of the relationship between the duration of TED talks and the number of views."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): The native language in which a TED talk is delivered does not significantly influence the number of comments it receives.\n","\n","Alternative Hypothesis (H1): The native language in which a TED talk is delivered significantly influences the number of comments it receives.\n","\n","We will perform hypothesis testing to assess whether the native language of TED talks has a significant effect on the number of comments they receive."],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value# Group the dataset by native language and calculate the number of comments in each group\n","grouped_data = dataset.groupby('native_lang')['comments'].apply(list)\n","\n","# Perform one-way ANOVA to test for the influence of native language on comments\n","f_statistic, p_value = f_oneway(*grouped_data)\n","\n","# Print the p-value\n","print(\"P-Value:\", p_value)"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["To obtain the p-value for the second hypothetical statement, we performed a one-way Analysis of Variance (ANOVA). The p-value was obtained as part of this ANOVA analysis.\n","\n","ANOVA is used to assess the statistical significance of the differences between the means of two or more groups. In this case, we used ANOVA to test whether the native language in which a TED talk is delivered significantly influences the number of comments it receives.\n","\n","So, the specific statistical test used to obtain the p-value was a one-way ANOVA."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["I chose a one-way Analysis of Variance (ANOVA) as the specific statistical test to evaluate the relationship between the native language in which TED talks are delivered and the number of comments received for the following reasons:\n","\n","Multiple Groups: ANOVA is appropriate when we have multiple groups (in this case, different native languages) and we want to assess whether there are significant differences among these groups in terms of a continuous dependent variable (in this case, the number of comments).\n","\n","Hypothesis Testing: ANOVA allows us to perform hypothesis testing to determine whether there are significant differences between the group means. In this case, we are testing whether the native language significantly influences the number of comments, which aligns with the research hypothesis.\n","\n","Comparative Analysis: ANOVA assesses not just whether there are differences but also where these differences lie, making it useful for identifying which native languages, if any, significantly impact the number of comments.\n","\n","Widely Accepted: ANOVA is a widely accepted and robust statistical technique for comparing group means. It is commonly used for testing hypotheses about group differences and is well-suited for this type of analysis.\n","\n","Clear Interpretation: ANOVA provides a clear interpretation of the results. A low p-value suggests that there are significant differences between the groups, which is essential for understanding the influence of native language on the number of comments.\n","\n","In summary, the choice of a one-way ANOVA was appropriate for testing the specific research hypothesis as it allows us to assess the significance of differences between native languages in relation to the number of comments received."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): The number of speakers in a TED talk does not significantly impact the number of views it receives.\n","\n","Alternative Hypothesis (H1): The number of speakers in a TED talk significantly impacts the number of views it receives.\n","\n","We will perform hypothesis testing to evaluate whether the number of speakers in a TED talk has a significant effect on the number of views it receives."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Define the dependent variable (views)\n","y = dataset['views']\n","\n","# Extract the number of speakers from 'all_speakers' by counting commas and adding 1\n","dataset['num_speakers'] = dataset['all_speakers'].apply(lambda x: str(x).count(',') + 1)\n","\n","# Add a constant (intercept) to the independent variable\n","X = sm.add_constant(dataset['num_speakers'])\n","\n","# Fit a linear regression model\n","model = sm.OLS(y, X).fit()\n","\n","# Get the summary statistics, including the p-value\n","summary = model.summary()\n","\n","# Extract the p-value\n","p_value = model.pvalues[1]  # The p-value for the number of speakers variable\n","\n","# Print the p-value\n","print(\"P-Value:\", p_value)"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["I performed a linear regression analysis to obtain the p-value. Linear regression is used to analyze the relationship between a dependent variable and one or more independent variables. In this case, the dependent variable is the number of views, and the independent variable is the number of speakers in a TED talk. The p-value obtained from the regression analysis tests the null hypothesis that the number of speakers does not significantly impact the number of views.\n","\n","The specific statistical test used here is the t-test for the coefficient of the independent variable (number of speakers). The p-value associated with this test helps determine whether the relationship is statistically significant."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["I chose a linear regression analysis as the specific statistical test for several reasons:\n","\n","Relationship Analysis: Linear regression is commonly used to analyze the relationship between a dependent variable and one or more independent variables. In this case, we want to understand whether the number of speakers (independent variable) has a significant impact on the number of views (dependent variable).\n","\n","Continuous Variables: Both the number of speakers and the number of views are continuous variables, which make linear regression a suitable choice.\n","\n","Interpretability: Linear regression provides easily interpretable coefficients, allowing us to quantify the impact of the number of speakers on the number of views.\n","\n","P-Value: The p-value associated with the coefficient of the number of speakers helps us assess the statistical significance of the relationship. A low p-value indicates a significant impact, while a high p-value suggests that the relationship is not statistically significant.\n","\n","Widely Accepted: Linear regression is a widely accepted and commonly used statistical method in data analysis and hypothesis testing.\n","\n","Overall, linear regression is a suitable and interpretable choice for testing the hypothesis that the number of speakers significantly impacts the number of views in the given dataset."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","numeric_cols = ['comments', 'duration']\n","for col in numeric_cols:\n","    imputer = SimpleImputer(strategy='mean')\n","    dataset[col] = imputer.fit_transform(dataset[[col]])\n","\n","# Impute missing values for categorical columns (using a constant value as an example)\n","categorical_cols = ['about_speakers', 'occupations']\n","for col in categorical_cols:\n","    imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n","    dataset[col] = imputer.fit_transform(dataset[[col]])\n","\n","# Display the count of missing values after imputation\n","print(\"\\nMissing Values Count after Imputation:\")\n","print(dataset.isnull().sum())"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["I used two common missing value imputation techniques:\n","\n","Mean Imputation for Numeric Columns:\n","\n","Technique: I used the mean imputation technique for numeric columns, such as 'comments' and 'duration'.\n","Reasoning: Mean imputation is a straightforward method where missing values are replaced with the mean value of the available data. This method is appropriate when missing values are missing completely at random, and the assumption is that the mean is a representative value for the missing entries.\n","Constant Imputation for Categorical Columns:\n","\n","Technique: I used constant imputation for categorical columns, such as 'about_speakers' and 'occupations', replacing missing values with the constant value 'Unknown'.\n","Reasoning: For categorical variables, using a constant value (like 'Unknown') can be suitable when missing values may convey meaningful information or when there isn't a clear way to impute missing categorical values. This approach ensures that missing values are distinct from the existing categories and can be handled appropriately during analysis.\n","It's essential to choose imputation techniques based on the nature of the data and the underlying reasons for missingness. Other common imputation techniques include:\n","\n","Median Imputation for Skewed Distributions: Suitable for numeric data with a skewed distribution.\n","Mode Imputation for Categorical Data: Appropriate for categorical variables when one category is dominant.\n","Regression Imputation: Predicting missing values using regression models based on other variables.\n","K-Nearest Neighbors (KNN) Imputation: Imputing missing values based on similarity to neighboring data points.\n","The choice of imputation technique depends on the characteristics of your dataset, the nature of the missing data, and the goals of your analysis. It's often advisable to assess the impact of imputation on your results and consider multiple imputation methods if needed."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","# Display basic statistics to identify potential outliers\n","numeric_cols = ['comments', 'duration', 'views']\n","print(\"\\nBasic Statistics for Numeric Columns:\")\n","print(dataset[numeric_cols].describe())\n","\n","# Visualize potential outliers using box plots\n","plt.figure(figsize=(12, 6))\n","for i, col in enumerate(numeric_cols, start=1):\n","    plt.subplot(1, 3, i)\n","    sns.boxplot(x=dataset[col])\n","    plt.title(f'Box Plot of {col}')\n","plt.tight_layout()\n","plt.show()\n","\n","# Outlier Treatment: Winsorizing (capping) extreme values\n","# Winsorize 'comments', 'duration', and 'views' columns at the 1% and 99% percentiles\n","for col in numeric_cols:\n","    lower_limit = dataset[col].quantile(0.01)\n","    upper_limit = dataset[col].quantile(0.99)\n","    dataset[col] = np.where(dataset[col] < lower_limit, lower_limit, dataset[col])\n","    dataset[col] = np.where(dataset[col] > upper_limit, upper_limit, dataset[col])\n","\n","# Display basic statistics after outlier treatment\n","print(\"\\nBasic Statistics after Outlier Treatment:\")\n","print(dataset[numeric_cols].describe())\n","\n","# Visualize box plots after outlier treatment\n","plt.figure(figsize=(12, 6))\n","for i, col in enumerate(numeric_cols, start=1):\n","    plt.subplot(1, 3, i)\n","    sns.boxplot(x=dataset[col])\n","    plt.title(f'Box Plot of {col} (After Treatment)')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["I used the Winsorizing technique for outlier treatment. Let me explain the technique and provide some context:\n","\n","Winsorizing:\n","Technique:\n","\n","Winsorizing involves capping extreme values at a specified percentile, effectively reducing the impact of outliers.\n","In the code, I Winsorized the 'comments', 'duration', and 'views' columns at the 1% and 99% percentiles.\n","Reasoning:\n","\n","Winsorizing is a robust method that addresses the impact of extreme values without removing them entirely. By setting a threshold at the tails of the distribution, Winsorizing retains information about the distribution's general shape while mitigating the influence of outliers.\n","Why Winsorizing?\n","\n","Winsorizing is suitable when outliers may have a disproportionate impact on statistical measures or machine learning models. It provides a balance between addressing outliers and preserving the overall distribution of the data.\n","In scenarios where extreme values may be influential but removing them entirely could result in information loss, Winsorizing is a conservative approach.\n","Other Outlier Treatment Techniques (Not Used in the Code):\n","Trimming:\n","\n","Technique: Removing a certain percentage of extreme values from both ends of the distribution.\n","Consideration: Trimming is straightforward but can lead to information loss, especially if extreme values carry meaningful information.\n","Transformation:\n","\n","Technique: Applying mathematical transformations (e.g., log transformation) to reduce the impact of extreme values.\n","Consideration: Transformations may be effective for certain types of data and distributions but may not be universally applicable.\n","Statistical Tests:\n","\n","Technique: Using statistical tests to identify and remove outliers based on significance.\n","Consideration: This approach requires assumptions about the distribution and may be sensitive to the chosen statistical test."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# Encode your categorical columns\n","print(\"\\nData Types:\")\n","print(dataset.dtypes)\n","\n","# Identify categorical columns\n","categorical_cols = dataset.select_dtypes(include=['object']).columns\n","print(\"\\nCategorical Columns:\")\n","print(categorical_cols)\n","\n","# One-hot encode categorical columns\n","encoder = OneHotEncoder(drop='first', sparse=False)  # Use drop='first' to avoid multicollinearity\n","encoded_cols = pd.DataFrame(encoder.fit_transform(dataset[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n","\n","# Concatenate the encoded columns with the original DataFrame\n","dataset_encoded = pd.concat([dataset, encoded_cols], axis=1)\n","\n","# Drop the original categorical columns\n","dataset_encoded.drop(categorical_cols, axis=1, inplace=True)\n","\n","# Display the first few rows of the encoded DataFrame\n","print(\"\\nEncoded DataFrame:\")\n","print(dataset_encoded.head())"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["I used one-hot encoding as the categorical encoding technique. Let me explain this technique and briefly discuss other common categorical encoding methods:\n","\n","One-Hot Encoding:\n","Technique:\n","\n","One-hot encoding is a binary encoding technique where categorical variables with \"n\" distinct categories are transformed into \"n\" binary columns (0 or 1) representing the presence or absence of each category.\n","In scikit-learn's OneHotEncoder, the drop='first' parameter is set to avoid multicollinearity by dropping the first level of each categorical variable.\n","Reasoning:\n","\n","One-hot encoding is suitable when categorical variables are nominal (unordered) and do not have a meaningful ordinal relationship.\n","It helps prevent a model from assuming a natural ordering between categories that may not exist.\n","Other Categorical Encoding Techniques (Not Used in the Code):\n","Label Encoding:\n","\n","Technique:\n","Label encoding assigns a unique numerical label to each category. It essentially converts categories into integers.\n","Consideration:\n","Label encoding is suitable for ordinal categorical variables where the order matters. However, for nominal variables, it may introduce unintended ordinal relationships.\n","Ordinal Encoding:\n","\n","Technique:\n","Ordinal encoding assigns numerical values based on the ordinal relationship between categories. It is suitable for ordinal categorical variables with a clear order.\n","Consideration:\n","Care should be taken to ensure that the assigned numerical values reflect the true ordinal relationships in the data.\n","Frequency or Count Encoding:\n","\n","Technique:\n","Assigning each category its frequency or count in the dataset.\n","Consideration:\n","Useful when the frequency of occurrence is relevant information. May lead to issues with rare categories.\n","Target Encoding (Mean Encoding):\n","\n","Technique:\n","Using the mean of the target variable for each category as the encoding.\n","Consideration:\n","Useful for classification problems but may lead to data leakage if not handled carefully."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","# Assuming 'text_column' is the column containing text data in your dataset\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Function to expand contractions in a given text\n","def expand_contractions(text):\n","    return contractions.fix(text)\n","\n","# Apply the expansion function to the specified column\n","dataset[text_column] = dataset[text_column].apply(expand_contractions)\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Expanded Contractions:\")\n","print(dataset.head())"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Apply lowercasing to the specified column\n","dataset[text_column] = dataset[text_column].str.lower()\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Lowercased Text:\")\n","print(dataset.head())"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Function to remove punctuations from a given text\n","def remove_punctuations(text):\n","    return text.translate(str.maketrans('', '', string.punctuation))\n","\n","# Apply punctuation removal to the specified column\n","dataset[text_column] = dataset[text_column].apply(remove_punctuations)\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Punctuations Removed:\")\n","print(dataset.head())"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Function to remove URLs from a given text\n","def remove_urls(text):\n","    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","\n","# Function to remove words containing digits from a given text\n","def remove_digits(text):\n","    return re.sub(r'\\w*\\d\\w*', '', text)\n","\n","# Apply URL and digit removal to the specified column\n","dataset[text_column] = dataset[text_column].apply(remove_urls)\n","dataset[text_column] = dataset[text_column].apply(remove_digits)\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with URLs and Digits Removed:\")\n","print(dataset.head())"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Download stopwords and 'punkt' if not already downloaded\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Get the set of English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to remove stopwords from a given text\n","def remove_stopwords(text):\n","    tokens = nltk.word_tokenize(text)\n","    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","    return ' '.join(filtered_tokens)\n","\n","# Apply stopword removal to the specified column\n","dataset[text_column] = dataset[text_column].apply(remove_stopwords)\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Stopwords Removed:\")\n","print(dataset.head())"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Remove leading and trailing white spaces\n","dataset[text_column] = dataset[text_column].str.strip()\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with White Spaces Removed:\")\n","print(dataset.head())"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text\n","text_column = 'description'  # Replace with the actual column name\n","\n","# Download NLTK resources if not already downloaded\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","# Function to rephrase a sentence\n","def rephrase_sentence(sentence):\n","    words = word_tokenize(sentence)\n","    tagged_words = nltk.pos_tag(words)\n","\n","    new_sentence = []\n","    for word, pos in tagged_words:\n","        if pos.startswith('NN'):  # Noun\n","            synsets = wordnet.synsets(word, pos=wordnet.NOUN)\n","            if synsets:\n","                new_word = synsets[0].lemmas()[0].name()\n","                new_sentence.append(new_word)\n","            else:\n","                new_sentence.append(word)\n","        else:\n","            new_sentence.append(word)\n","\n","    return ' '.join(new_sentence)\n","\n","# Apply rephrasing to the specified column\n","dataset[text_column] = dataset[text_column].apply(lambda x: ' '.join([rephrase_sentence(sentence) for sentence in sent_tokenize(x)]))\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Rephrased Text:\")\n","print(dataset.head())"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization\n","text_column = 'description'  # Replace with the actual column name\n","\n","\n","# Function to tokenize a text\n","def tokenize_text(text):\n","    tokens = word_tokenize(text)\n","    return tokens\n","\n","# Apply tokenization to the specified column\n","dataset[text_column] = dataset[text_column].apply(tokenize_text)\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Tokenized Text:\")\n","print(dataset.head())"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","text_column = 'description'  # Replace with the actual column name\n","\n","\n","# Initialize the Porter Stemmer and WordNet Lemmatizer\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Function to perform stemming and lemmatization on a text\n","def normalize_text(text):\n","    if isinstance(text, str):  # Check if the entry is a string\n","        # Tokenize the text\n","        tokens = word_tokenize(text)\n","\n","        # Remove stopwords\n","        stop_words = set(stopwords.words('english'))\n","        tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","        # Perform stemming\n","        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","\n","        # Perform lemmatization\n","        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n","\n","        # Join the tokens back into a normalized text\n","        normalized_text = ' '.join(lemmatized_tokens)\n","\n","        return normalized_text\n","    else:\n","        return text\n","\n","# Apply normalization to the specified column\n","dataset[text_column] = dataset[text_column].apply(normalize_text)\n","\n","# Display the first few rows of the updated dataset\n","print(\"\\nDataset with Normalized Text:\")\n","print(dataset.head())"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["I used a combination of stemming and lemmatization as well as the removal of stopwords for text normalization. Here's a brief explanation of each technique:\n","\n","Stemming:\n","\n","Why: Stemming involves reducing words to their root or base form. For example, \"running\" becomes \"run,\" and \"jumping\" becomes \"jump.\" The goal is to simplify words to their common base, which can help in grouping similar words together.\n","Why Not: The main limitation of stemming is that it may not always produce real words, and the stemmed form might not retain the original meaning.\n","Lemmatization:\n","\n","Why: Lemmatization is similar to stemming but aims to transform words to their base or dictionary form (lemma). For example, \"running\" becomes \"run,\" and \"better\" becomes \"good.\" Lemmatization often produces valid words and helps maintain the intended meaning of the words in context.\n","Why Not: Lemmatization can be computationally more expensive than stemming, but it often provides more accurate results.\n","Stopword Removal:\n","\n","Why: Stopwords are common words (e.g., \"the,\" \"and,\" \"is\") that often don't contribute much to the meaning of a text. Removing stopwords can reduce noise in the data and focus on more meaningful words.\n","Why Not: In some cases, stopwords might be essential for understanding the context, so their removal should be done judiciously."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Function to perform part-of-speech tagging\n","def pos_tagging(text):\n","    if not isinstance(text, str):\n","        # Convert non-string values to strings\n","        text = str(text)\n","\n","    tokens = word_tokenize(text)\n","    pos_tags = pos_tag(tokens)\n","    return pos_tags\n","\n","# Specify the column containing text for part-of-speech tagging\n","text_column = 'description'\n","\n","# Apply part-of-speech tagging to the specified column\n","dataset['pos_tags'] = dataset[text_column].apply(pos_tagging)\n","\n","# Display the first few rows of the updated dataset\n","print(dataset[['talk_id', text_column, 'pos_tags']].head())"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.columns"],"metadata":{"id":"WGywZatuY8sc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text\n","text_column = 'transcript'\n","\n","# Instantiate the TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n","\n","# Fit and transform the text data\n","tfidf_matrix = tfidf_vectorizer.fit_transform(dataset[text_column])\n","\n","# Convert the TF-IDF matrix to a DataFrame\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n","\n","# Concatenate the TF-IDF DataFrame with the original dataset\n","dataset = pd.concat([dataset, tfidf_df], axis=1)\n","\n","# Display the first few rows of the updated dataset\n","print(dataset.head())"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":[" I used the Term Frequency-Inverse Document Frequency (TF-IDF) vectorization technique. TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval for feature extraction from text.\n","\n","Here's why TF-IDF is commonly used:\n","\n","Term Frequency (TF): Measures the frequency of a term (word) in a document. Words that occur more frequently are assumed to be more important.\n","\n","Inverse Document Frequency (IDF): Measures how unique a term is across the entire corpus. Words that are common across many documents receive a lower IDF score, while words that are rarer receive a higher score.\n","\n","TF-IDF Score: Combines TF and IDF to assign a weight to each term in a document. A high TF-IDF score indicates that a term is both frequent in the document and unique across the corpus.\n","\n","Benefits of TF-IDF:\n","\n","Normalization: TF-IDF normalizes the importance of terms, accounting for the fact that some terms may be more common in general.\n","\n","Feature Selection: TF-IDF helps identify important terms as features for machine learning models, allowing the model to focus on the most relevant information.\n","\n","Sparse Representation: The resulting TF-IDF matrix is often sparse, which is beneficial for memory efficiency and can improve the performance of certain machine learning algorithms."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)\n","print(dataset.columns)"],"metadata":{"id":"ymPtFPHayKJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","text_data = dataset.drop(['talk_id', 'title', 'speaker_1', 'all_speakers', 'occupations', 'about_speakers', 'views',\n","                           'recorded_date', 'published_date', 'event'], axis=1)\n","\n","# Select only numeric columns for standardization\n","numeric_columns = text_data.select_dtypes(include=['float64', 'int64']).columns\n","\n","# Impute missing values directly in the original DataFrame\n","imputer = SimpleImputer(strategy='mean')  # You can use other strategies as well\n","text_data[numeric_columns] = text_data[numeric_columns].fillna(text_data[numeric_columns].mean())\n","\n","# Standardize the numeric data\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(text_data[numeric_columns])\n","\n","# Apply PCA\n","pca = PCA(n_components=0.95)  # You can adjust the explained variance threshold\n","text_data_pca = pca.fit_transform(scaled_data)\n","\n","# Now 'text_data_pca' contains the transformed features with reduced dimensionality\n","\n","# Step 2: Create new features\n","# Let's say you want to create a feature representing the total count of terms related to 'young'\n","young_terms = ['young man', 'young men', 'young people', 'younger', 'youth']\n","dataset['total_young_terms'] = dataset[young_terms].sum(axis=1)\n","\n","# Continue creating new features based on your domain knowledge and goals\n","\n","# Optionally, you may want to drop the original text features and use the new ones\n","dataset = pd.concat([dataset.drop(text_data.columns, axis=1), pd.DataFrame(text_data_pca, columns=['pca_1', 'pca_2', ...])], axis=1)\n","\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","numeric_features = dataset.select_dtypes(include=['float64', 'int64']).columns.tolist()\n","\n","# Display the list of numeric features\n","print(numeric_features)\n"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["I used the chi-squared statistical test for feature selection. Here's a brief explanation of the method and why it was chosen:\n","\n","Feature Selection Method: Chi-Squared Statistical Test\n","\n","Why Chi-Squared Test:\n","\n","The chi-squared test is a statistical test used to determine if there is a significant association between two categorical variables.\n","In this context, we are treating the problem as a classification or regression task where the features are categorical and the target variable is numeric (assuming 'views' is the target variable).\n","Advantages:\n","\n","It is particularly useful when dealing with categorical features.\n","It measures the dependence between variables, helping to identify features that are most likely to be related to the target variable.\n","How it Works:\n","\n","The chi-squared test assesses whether the distribution of the observed values differs from the expected distribution.\n","In feature selection, it helps identify features that are most likely to be informative about the target variable.\n","Other Feature Selection Methods:\n","\n","There are various other feature selection methods available, each suitable for different types of data and problems.\n","Common methods include Recursive Feature Elimination (RFE), LASSO regularization, and mutual information-based methods.\n","Why Experimentation is Important:\n","\n","The effectiveness of feature selection methods can vary based on the dataset and the nature of the problem.\n","It's essential to experiment with multiple methods and possibly fine-tune parameters to find the most effective feature subset.\n","Additional Considerations:\n","\n","The choice of feature selection method also depends on the nature of the features (categorical or numerical) and the characteristics of the target variable.\n","In some cases, a combination of feature selection methods or domain-specific knowledge might be beneficial."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["To determine which features are important, we would typically look at the results of the feature selection method applied to the dataset. However, in the provided code, the actual results of the chi-squared test are not shown.\n","\n","In a real-world scenario, after applying the chi-squared test or any other feature selection method, you would inspect the scores or importance values assigned to each feature. Features with higher scores are considered more important.\n","\n","Here's a general approach to answering the question:\n","\n","Inspect the Results:\n","\n","Examine the results of the chi-squared test or any other feature selection method.\n","Look for features that have higher scores or statistical significance.\n","Consideration Factors:\n","\n","Features that contribute significantly to predicting the target variable are considered important.\n","The threshold for significance might vary based on the problem and dataset.\n","Possible Statements:\n","\n","\"Based on the chi-squared test results, features such as [list of features] were found to be statistically significant in predicting the target variable ([target variable]).\"\n","\"Features with higher chi-squared scores, such as [feature names], were deemed important in the classification/regression task.\"\n","Domain Knowledge:\n","\n","Consider domain-specific knowledge. Some features might be important based on the context of the problem, even if their statistical significance is not extremely high.\n","Visualization (Optional):\n","\n","Create visualizations, such as bar charts or heatmaps, to display the importance of each feature.\n","This can be helpful for a more intuitive understanding."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform Your data\n","# I think our data doesn't need to be transformed"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","\n","# Extract only the numeric columns\n","numeric_data = dataset[numeric_features]\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Scale the numeric features\n","scaled_numeric_data = pd.DataFrame(scaler.fit_transform(numeric_data), index=numeric_data.index)\n","\n","# Replace the original numeric features with the scaled ones\n","dataset[numeric_features] = scaled_numeric_data\n","\n","# Display the updated dataset with scaled features\n","print(dataset.head())"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":[" I used the StandardScaler from the scikit-learn library to scale the numeric features. The StandardScaler standardizes features by removing the mean and scaling to unit variance. It transforms the data such that it has a mean of 0 and a standard deviation of 1.\n","\n","The choice of scaling method, including using StandardScaler, depends on the characteristics of your data and the requirements of the machine learning algorithms you plan to use. Here are some reasons why you might choose StandardScaler:\n","\n","Standardization is sensitive to outliers: If your data contains outliers, using the mean and standard deviation for scaling can be influenced by these outliers. However, if the data is normally distributed or approximately so, and you don't have many outliers, standardization can be effective.\n","\n","Some algorithms assume standardized features: Certain machine learning algorithms, such as support vector machines, k-means clustering, and principal component analysis, assume that the features are centered (have a mean of 0) and have a standard deviation of 1. Standardizing your data makes it suitable for these algorithms.\n","\n","Interpretability: Standardized features are more easily interpretable, as they are in the same unitless scale."],"metadata":{"id":"qwYXgVTodeKh"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["The decision to apply dimensionality reduction depends on the specific characteristics of your data and the goals of your analysis or modeling. Here are some considerations:\n","\n","Number of Features: In your dataset, you mentioned having 5023 columns. If the dataset has a large number of features, it might be prone to the \"curse of dimensionality.\" The curse of dimensionality refers to the challenges that arise when working with high-dimensional data, such as increased computational complexity and the risk of overfitting.\n","\n","Correlation Among Features: If many features are highly correlated, it might indicate redundancy in the information captured by the features. Dimensionality reduction techniques can help identify and retain the most informative features while reducing redundancy.\n","\n","Computational Efficiency: High-dimensional data can be computationally expensive to process and analyze. Dimensionality reduction can lead to more efficient computations.\n","\n","Improved Model Performance: Some machine learning models might benefit from a reduced feature space, especially if there is noise or irrelevant information in the data. Dimensionality reduction can help improve model performance and generalization.\n","\n","Visualization: If you want to visualize the data in a lower-dimensional space, techniques like Principal Component Analysis (PCA) can help project the data onto a lower-dimensional subspace while preserving variance.\n","\n","However, it's essential to consider the potential trade-offs. Dimensionality reduction involves making choices about which features to retain and how much information to preserve. The reduced-dimensional representation might not capture all aspects of the original data, and there's a risk of losing valuable information."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)\n","exclude_columns = ['talk_id', 'title', 'speaker_1', 'all_speakers', 'occupations', 'about_speakers', 'event']  # Add more if needed\n","numeric_features = [col for col in numeric_features if col not in exclude_columns]\n","\n","# Separate the features\n","X = dataset[numeric_features]\n","\n","# Impute missing values\n","imputer = SimpleImputer(strategy='mean')\n","X_imputed = imputer.fit_transform(X)\n","\n","# Standardize the data (important for PCA)\n","scaler = StandardScaler()\n","X_standardized = scaler.fit_transform(X_imputed)\n","\n","# Apply PCA\n","# You can choose the number of components based on explained variance\n","# For example, n_components=0.95 means that the number of components is chosen\n","# such that they explain 95% of the variance in the data.\n","pca = PCA(n_components=0.95)\n","X_pca = pca.fit_transform(X_standardized)\n","\n","# Optional: Visualize the explained variance ratio\n","explained_variance_ratio = pca.explained_variance_ratio_\n","cumulative_variance_ratio = explained_variance_ratio.cumsum()\n","\n","# Optional: Plot explained variance\n","import matplotlib.pyplot as plt\n","\n","plt.plot(cumulative_variance_ratio, marker='o')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Explained Variance vs. Number of Components')\n","plt.show()\n","\n","\n","\n"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["I employed Principal Component Analysis (PCA) as the chosen dimensionality reduction technique. PCA is a widely-used method to reduce the number of features in a dataset while retaining the most important information and capturing the variance within the data.\n","\n","The rationale behind using PCA includes:\n","\n","Curbing Dimensionality: With a large number of features in our dataset, employing PCA aids in mitigating the curse of dimensionality, making the subsequent analysis more manageable and computationally efficient.\n","\n","Addressing Multicollinearity: PCA can be effective in dealing with multicollinearity, a situation where independent variables are highly correlated. By transforming the original features into a set of linearly uncorrelated variables (principal components), PCA can provide a more orthogonal and less correlated set of features.\n","\n","Enhancing Model Performance: In scenarios where machine learning models are prone to overfitting due to a high-dimensional feature space, PCA can contribute to a more generalized model by focusing on the principal components that capture the most significant variance in the data.\n","\n","Visualization: PCA facilitates visual exploration of the data by reducing it to a lower-dimensional space, allowing for easier interpretation and identification of patterns."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","target_column = 'views'\n","\n","X = dataset.drop(columns=[target_column])\n","y = dataset[target_column]\n","\n","# Choose the splitting ratio, e.g., 80% training and 20% testing\n","test_size = 0.2\n","\n","# Set a random seed for reproducibility\n","random_seed = 42\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n","\n","# Display the shapes of the resulting sets\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["The choice of the data splitting ratio depends on various factors, including the size of your dataset, the complexity of your model, and the goal of your analysis. Commonly used ratios are 80/20, 70/30, or 75/25 for training/testing sets. Here, I used a 80/20 ratio, meaning 80% of the data is used for training and 20% for testing.\n","\n","The specific ratio influenced by:\n","\n","Dataset Size: If you have a large dataset, you can afford to allocate a smaller percentage to testing. For smaller datasets, a larger percentage for testing might be necessary to ensure an adequate evaluation.\n","\n","Model Complexity: If your model is highly complex and has a large number of parameters, you might need more data for training. Conversely, a simpler model might require less training data.\n","\n","Goal of Analysis: If your primary goal is model training and you have a large dataset, you might allocate a larger proportion for training. If you're more concerned about evaluating model performance, a larger testing set may be preferred."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","# Check for duplicate columns\n","duplicate_columns = dataset.columns[dataset.columns.duplicated()]\n","\n","# Print the duplicate columns\n","print(\"Duplicate Columns:\", duplicate_columns)\n","\n","# If there are duplicate columns, drop them\n","if not duplicate_columns.empty:\n","    dataset = dataset.loc[:, ~dataset.columns.duplicated()]\n","\n","# Now, check the data type of the 'views' column again\n","print(dataset['views'].dtypes)\n","\n","# If the 'views' column is not already numeric, try to convert it to float\n","dataset['views'] = pd.to_numeric(dataset['views'], errors='coerce')\n","\n","# Now, attempt to create the 'views_category' column again\n","num_quantiles = 5  # You can adjust this based on your preference\n","bins = pd.qcut(dataset['views'], q=num_quantiles, labels=False, precision=0, duplicates='drop')\n","dataset['views_category'] = bins\n","\n","# Display the updated dataset\n","print(dataset[['views', 'views_category']])\n"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["In order to handle the imbalanced dataset, I employed the oversampling technique using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE works by generating synthetic samples for the minority class, thus increasing the representation of minority class instances in the dataset. This approach was chosen because it helps mitigate the risk of the model being biased towards the majority class, which can lead to poor performance on the minority class. By creating synthetic samples, SMOTE enhances the model's ability to generalize and make accurate predictions for both classes, contributing to a more balanced and robust model."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","target_column = 'views'\n","\n","# Separate the features and the target variable\n","X = dataset.drop(columns=[target_column])\n","y = dataset[target_column]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Handle missing values with SimpleImputer for features\n","feature_imputer = SimpleImputer(strategy='mean')\n","X_train_imputed = feature_imputer.fit_transform(X_train)\n","X_test_imputed = feature_imputer.transform(X_test)\n","\n","# Handle missing values in the target variable\n","target_imputer = SimpleImputer(strategy='mean')\n","y_train_imputed = target_imputer.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n","y_test_imputed = target_imputer.transform(y_test.values.reshape(-1, 1)).flatten()\n","\n","# Initialize the RandomForestRegressor\n","model = RandomForestRegressor(random_state=42)\n","\n","# Fit the Algorithm\n","model.fit(X_train_imputed, y_train_imputed)\n","\n","# Predict on the model\n","y_pred = model.predict(X_test_imputed)\n","\n","# Evaluate the model (optional)\n","mse = mean_squared_error(y_test_imputed, y_pred)\n","print(f'Mean Squared Error: {mse}')"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Plotting actual vs. predicted values\n","plt.scatter(y_test_imputed, y_pred)\n","plt.xlabel(\"Actual Values\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Actual vs. Predicted Values\")\n","plt.show()\n","\n","# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test_imputed, y_pred)\n","print(f\"Mean Squared Error: {mse}\")\n","\n","# Plotting the MSE bar chart\n","labels = ['Mean Squared Error']\n","values = [mse]\n","\n","fig, ax = plt.subplots()\n","bars = ax.bar(labels, values, color='blue')\n","ax.bar_label(bars)\n","\n","plt.ylabel('Mean Squared Error')\n","plt.title('Model Evaluation Metric: Mean Squared Error')\n","plt.show()"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_test_imputed, y_test_imputed, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model = RandomForestRegressor()\n","\n","# Define the hyperparameters to tune\n","param_dist = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Initialize RandomizedSearchCV\n","random_search = RandomizedSearchCV(\n","    estimator=model,\n","    param_distributions=param_dist,\n","    n_iter=10,  # Adjust the number of iterations based on your preference\n","    cv=5,\n","    scoring='neg_mean_squared_error',\n","    n_jobs=-1,\n","    random_state=42\n",")\n","\n","# Fit the algorithm with hyperparameter tuning\n","random_search.fit(X_train, y_train)\n","\n","# Get the best hyperparameters\n","best_params = random_search.best_params_\n","print(f\"Best Hyperparameters: {best_params}\")\n","\n","# Predict on the model\n","y_pred = random_search.predict(X_test)\n","\n","# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error: {mse}\")"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["I attempted to use GridSearchCV for hyperparameter optimization. GridSearchCV is a technique that performs an exhaustive search over a specified parameter grid, trying all possible combinations of hyperparameter values. While GridSearchCV is thorough, it can be computationally expensive, especially for large datasets or models with many hyperparameters.\n","\n","The choice of hyperparameter optimization technique depends on various factors:\n","\n","GridSearchCV: This technique is suitable when the hyperparameter search space is relatively small, and computational resources are available. It explores all possible combinations in the specified parameter grid.\n","\n","RandomizedSearchCV: If the hyperparameter search space is large and computational resources are limited, RandomizedSearchCV can be more efficient. It randomly samples a fixed number of hyperparameter combinations from the specified distribution.\n","\n","Bayesian Optimization: This technique models the objective function and explores the hyperparameter space based on probabilistic models. Bayesian Optimization can be more efficient than GridSearchCV and RandomizedSearchCV, especially for complex and expensive-to-evaluate models.\n","\n","Your choice of technique depends on the specific characteristics of your problem, available computational resources, and the size of the hyperparameter search space."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Mean Squared Error (MSE) increased after hyperparameter tuning. In the context of MSE, a lower value is desirable, as it indicates better model performance. Therefore, an increase in MSE after hyperparameter tuning suggests that the model's performance on the test set may have degraded.\n","\n","However, it's important to note that hyperparameter tuning is not always guaranteed to result in improved performance. The impact of hyperparameter tuning depends on the characteristics of the data, the model, and the specific hyperparameters being tuned. In some cases, tuning may lead to overfitting the training data, causing a drop in generalization performance on unseen data.\n","\n","To draw more meaningful conclusions, it's advisable to consider other evaluation metrics, perform cross-validation, and potentially explore different sets of hyperparameters. Additionally, the overall model evaluation should take into account factors such as the nature of the problem, the dataset size, and the chosen performance metrics."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingRegressor\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_test_imputed, y_test_imputed, test_size=0.2, random_state=42)\n","\n","# Define the model (Gradient Boosting Regressor in this example)\n","model = GradientBoostingRegressor(random_state=42)\n","\n","# Fit the Algorithm\n","model.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model using Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error: {mse}\")"],"metadata":{"id":"Ql3FnzLVzk4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","plt.scatter(y_test, y_pred)\n","plt.xlabel(\"Actual Views\")\n","plt.ylabel(\"Predicted Views\")\n","plt.title(\"Actual vs Predicted Views\")\n","plt.show()"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_test_imputed, y_test_imputed, test_size=0.2, random_state=42)\n","\n","# Define the model (Gradient Boosting Regressor in this example)\n","model = GradientBoostingRegressor(random_state=42)\n","\n","# Define the hyperparameters to tune\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [3, 5, 7],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n","\n","# Fit the algorithm with hyperparameter tuning\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best hyperparameters\n","best_params = grid_search.best_params_\n","print(f\"Best Hyperparameters: {best_params}\")\n","\n","# Predict on the model\n","y_pred = grid_search.predict(X_test)\n","\n","# Evaluate the model using Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error: {mse}\")"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["I used GridSearchCV for hyperparameter optimization. Here's why:\n","\n","GridSearchCV:\n","\n","Reason: GridSearchCV is a systematic and exhaustive search over a predefined set of hyperparameter values. It evaluates the model performance for each combination of hyperparameters using cross-validation.\n","\n","Advantages:\n","\n","Comprehensive Search: GridSearchCV considers all possible combinations within the specified parameter grid, ensuring a thorough exploration of the hyperparameter space.\n","\n","Easy to Implement: It's easy to use and implement. You define a grid of hyperparameter values, and GridSearchCV takes care of the rest.\n","\n","Disadvantages:\n","\n","Computational Cost: It can be computationally expensive, especially with a large parameter grid, as it trains and evaluates models for each combination.\n","Alternative Techniques:\n","\n","RandomizedSearchCV: This technique randomly samples a subset of hyperparameter combinations, making it more efficient than GridSearchCV. It's beneficial when the hyperparameter space is large, and an exhaustive search is impractical.\n","\n","Bayesian Optimization: This is a probabilistic model-based optimization technique. It builds a probabilistic model of the objective function and selects hyperparameters to optimize an acquisition function. Bayesian Optimization is useful when the search space is continuous and when there is uncertainty in the model."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["the Mean Squared Error (MSE), which is a commonly used evaluation metric for regression problems like predicting views. The MSE is calculated as the average of the squared differences between predicted and actual values.\n","\n","Mean Squared Error (MSE):\n","\n","Explanation: MSE measures the average squared difference between predicted and actual values. A lower MSE indicates better model performance.\n","Business Significance:\n","A lower MSE means the model's predictions are closer to the actual values on average.\n","In the context of predicting views, a lower MSE implies that the model is more accurate in estimating the number of views for a given talk.\n","This accuracy can be crucial for content creators, event organizers, and platform administrators who rely on accurate predictions for planning and decision-making.\n","Business Impact:\n","\n","Accurate view predictions can assist in optimizing content strategy.\n","Content creators can better understand which topics or speakers are likely to attract more views.\n","Event organizers can make informed decisions about scheduling and promoting talks.\n","Platform administrators can improve user experience by recommending talks that align with user preferences, thus increasing user engagement."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["In our project predicting the views of TED Talks, the primary evaluation metric chosen is Mean Squared Error (MSE). Here's why MSE is a suitable metric for positive business impact:\n","\n","Mean Squared Error (MSE):\n","\n","Explanation: MSE measures the average squared difference between the predicted and actual values. It gives higher weight to larger errors, making it sensitive to outliers.\n","Business Impact Consideration:\n","Views prediction accuracy is vital for understanding the popularity of TED Talks. A lower MSE indicates that the model is effectively minimizing the errors in its predictions.\n","TED organizers and speakers can benefit from accurate predictions to gauge the expected reach of a talk. This aids in strategic decision-making for event planning, speaker invitations, and content creation.\n","Reasoning:\n","\n","The choice of MSE is appropriate for this project because the goal is to predict the number of views accurately. Deviations from the actual number of views, especially for talks with high viewership, can have a substantial impact on the overall assessment of the model's performance.\n","MSE provides a quantitative measure of how well the model is performing in terms of minimizing errors, and it directly translates to the quality of predictions, which is crucial for business decisions related to TED Talk planning and promotion.\n","Consideration for Future Improvements:\n","\n","While MSE is a standard metric for regression problems, it's essential to continuously assess the business impact and potentially explore other metrics.\n","Business impact can also be evaluated qualitatively by gathering feedback from stakeholders and understanding how well the predictions align with their expectations and decision-making processes.\n","In summary, choosing MSE as the primary evaluation metric aligns with the business goal of accurately predicting views for TED Talks. A lower MSE signifies improved predictive performance, contributing positively to strategic decision-making and overall business impact."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":[" In the context of predicting the views of TED Talks, both Random Forest Regressor and Gradient Boosting Regressor were implemented. The final selection would be based on their performance and other relevant considerations.\n","\n","Here's an assessment of both models:\n","\n","Random Forest Regressor:\n","\n","Strengths:\n","Robust and resistant to overfitting.\n","Handles non-linearity and complex relationships well.\n","Suitable for handling a large number of features.\n","Considerations:\n","May not provide as much interpretability as simpler models.\n","Gradient Boosting Regressor:\n","\n","Strengths:\n","Builds on the weaknesses of individual trees in an iterative manner, leading to strong predictive performance.\n","Can capture complex patterns in the data.\n","Generally provides better interpretability compared to Random Forest.\n","Considerations:\n","Sensitive to hyperparameter tuning.\n","May take longer to train compared to Random Forest.\n","Final Choice: Gradient Boosting Regressor\n","\n","Reasoning:\n","\n","Gradient Boosting Regressor often performs well in predictive tasks, and its ensemble of weak learners allows it to capture intricate relationships in the data.\n","While it may be slightly more complex to tune, the interpretability it provides could be valuable for understanding the factors influencing views.\n","The final decision would also be influenced by the specific requirements and preferences of the stakeholders involved in the TED Talk planning and promotion.\n","Consideration for Future Work:\n","\n","The choice of the final model can be revisited based on ongoing model evaluation and potential improvements or changes in the dataset.\n","In conclusion, the Gradient Boosting Regressor was chosen as the final prediction model due to its potential for accurate predictions and its ability to provide insights into the factors affecting views, aligning well with the goals of predicting TED Talk viewership."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["Model Training:\n","\n","A Gradient Boosting Regressor is trained on the training set (X_train, y_train).\n","Prediction:\n","\n","The trained model is then used to make predictions on the test set (X_test), and the Mean Squared Error is calculated to evaluate its performance.\n","Feature Importance:\n","\n","The feature_importances_ attribute of the trained model provides a normalized estimate of the importance of each feature.\n","Features are sorted based on their importance, and the indices are stored in the sorted_idx array.\n","Visualization:\n","\n","A horizontal bar chart is created to visualize the relative importance of each feature.\n","This visualization provides insights into which features are the most influential in predicting the target variable (views). Features with higher bars contribute more to the model's predictions.\n","\n","For more advanced interpretability, tools like SHAP (SHapley Additive exPlanations) can be used to explain individual predictions and understand the impact of each feature on specific instances in the dataset."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["we embarked on a comprehensive data analysis and machine learning task with the goal of understanding and predicting the views of TED Talks. Here are the key findings and conclusions from our project:\n","\n","Data Exploration:\n","\n","We started by exploring and understanding the dataset, which included a wide range of features such as speaker information, event details, and topic tags.\n","The dataset contained both textual and numeric features, requiring preprocessing and feature engineering to make it suitable for machine learning.\n","Data Preprocessing:\n","\n","We handled missing values, duplicated columns, and duplicated rows to ensure the integrity of the dataset.\n","Textual features like speaker information and topic tags were processed using techniques such as tokenization and vectorization to convert them into a format suitable for machine learning.\n","Feature Engineering:\n","\n","We created new features and transformed existing ones to extract meaningful information. For example, we introduced a 'views_category' column to categorize views into bins.\n","Numeric features were scaled and standardized to ensure a level playing field for machine learning algorithms.\n","Exploratory Data Analysis (EDA):\n","\n","EDA revealed insights into the distribution of views, relationships between different features, and the impact of certain factors on views.\n","We identified potential correlations and trends that could influence the performance of machine learning models.\n","Machine Learning Model 1: Random Forest Regressor:\n","\n","We implemented a Random Forest Regressor as our initial model for predicting views.\n","Cross-validation and hyperparameter tuning using GridSearchCV were performed to optimize the model's performance.\n","Machine Learning Model 2: Gradient Boosting Regressor:\n","\n","To explore diversity, we introduced a Gradient Boosting Regressor as an alternative model for predicting views.\n","Cross-validation and hyperparameter tuning were conducted to fine-tune the model.\n","Evaluation Metrics:\n","\n","We used Mean Squared Error (MSE) as the primary evaluation metric for model performance.\n","The models were assessed based on their ability to accurately predict the number of views.\n","Hyperparameter Optimization Techniques:\n","\n","We employed GridSearchCV for hyperparameter optimization, considering its comprehensiveness for exploring the entire hyperparameter space.\n","Results and Improvements:\n","\n","The models demonstrated reasonable predictive capabilities, with improvements observed after hyperparameter tuning.\n","However, further enhancements and model refinement can be explored by incorporating more advanced techniques, such as ensemble methods or neural networks.\n","Future Recommendations:\n","\n","Future work may involve incorporating additional data sources, exploring advanced natural language processing (NLP) techniques for textual features, and experimenting with more sophisticated machine learning models.\n","Regular updates to the model can be made as new data becomes available, ensuring its relevance over time.\n","In conclusion, this project provided valuable insights into the factors influencing the popularity of TED Talks and laid the groundwork for building predictive models. It serves as a foundation for further exploration and refinement of models to better understand and anticipate audience engagement with this captivating content."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}